{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Helsinki, Master's Programme in Data Science  \n",
    "DATA20019 Trustworthy Machine Learning, Autumn 2019  \n",
    "Antti Honkela and Razane Tajeddine  \n",
    "\n",
    "# Project 2: Real-life privacy-preserving machine learning\n",
    "\n",
    "Deadline for returning the solutions: 24 November 23:55.\n",
    "\n",
    "## General instructions (IMPORTANT!)\n",
    "\n",
    "1. This is an individual project. You can discuss the solutions with other students, but everyone needs to write their own code and answers.\n",
    "2. Please return your solutions as a notebook. When returning your solutions, please leave all output in the notebook.\n",
    "3. When returning your solutions, please make sure the notebook can be run cleanly using \"Cell\" / \"Run All\".\n",
    "4. Please make sure there are no dependencies between solutions to different problems.\n",
    "5. Please make sure that your notebook will not depend on any local files.\n",
    "6. Please make sure that the solutions for each problem in your notebook will produce the same results when run multiple times, i.e. remember to seed any random number generators you use (`numpy.random.seed()`!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Differentially private logistic regression with DP-SGD and synthetic data\n",
    "\n",
    "TensorFlow Privacy (https://github.com/tensorflow/privacy) library provides implementations of many differentially private optimisation algorithms for deep learning and other models. In order to perform these exercises, you will need to install TensorFlow Privacy and its dependencies according to instructions given on the website.\n",
    "\n",
    "In order to study TensorFlow Privacy, we will use logistic regression on a small synthetic data set. This will be faster to run than larger neural network models. A simple example implementation of the model is available at https://github.com/ahonkela/privacy/blob/master/tutorials/toy_lr_tutorial.py\n",
    "The code has been adapted from tutorials provided with TensorFlow privacy.\n",
    "\n",
    "The definition of the logistic regression model binary classification is itself very straightforward in TensorFlow, simply using a single fully connected linear layer with cross entropy loss:\n",
    "```{python}\n",
    "  # Define logistic regression model using tf.keras.layers.\n",
    "  logits = tf.keras.layers.Dense(2).apply(features['x'])\n",
    "\n",
    "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
    "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits)\n",
    "```\n",
    "\n",
    "The rest of the example provides supporting architecture. Key parameters of the algorithm are defined as `flags` at the beginning of the file. These include:\n",
    "```{python}\n",
    "flags.DEFINE_float('learning_rate', .05, 'Learning rate for training')\n",
    "flags.DEFINE_float('noise_multiplier', 2.0,\n",
    "                   'Ratio of the standard deviation to the clipping norm')\n",
    "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
    "flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 2, 'Number of epochs')\n",
    "```\n",
    "\n",
    "`learning_rate` is the initial learning rate for the Adam optimiser. Larger value means faster learning but can cause instability.  \n",
    "`noise_multiplier` controls the amount of noise added in DP-SGD: higher value means more noise. The value is defined relative to the gradient clipping norm.  \n",
    "`l2_norm_clip` is the maximum norm at which per-example gradients are clipped. Smaller values mean less noise with the same level of privacy, but too small values can bias the results and make learning impossible.  \n",
    "`batch_size` is the minibatch size which impacts privacy via amplification from subsampling. Smaller batch sizes increase privacy for equal number of epochs, but too small batches can make the learning unstable.  \n",
    "`epochs` controls the length of training as a number of passes over the entire data.\n",
    "\n",
    "Test how these parameters (clipping threshold, batch size, noise multiplier and learning rate) affect the accuracy or the classifier and the privacy. Plot all your results to a privacy ($\\epsilon$) vs. accuracy plot to trace the optimal accuracy achievable under a specific level of privacy.\n",
    "\n",
    "You can limit the number of experiments to keep the runtimes reasonable: it is not necessary to try every combination of parameters but you can focus on testing the effect of one variable at a time. TensorFlow can use GPUs which can speed up learning significantly.\n",
    "\n",
    "Note: testing several hyperparameters and choosing the best has an impact on the privacy guarantees. There are methods for dealing with this (e.g. https://arxiv.org/abs/1811.07971) but the field is still under active development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kokutvon/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-11-24 11:49:23.976139: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "For delta=1e-5, the current epsilon is: 0.47\n",
      "For delta=1e-5, the current epsilon is: 0.49\n",
      "For delta=1e-5, the current epsilon is: 0.52\n",
      "For delta=1e-5, the current epsilon is: 0.54\n",
      "For delta=1e-5, the current epsilon is: 0.56\n",
      "Test accuracy after 0.5 epochs is: 0.833\n",
      "For delta=1e-5, the current epsilon is: 0.59\n",
      "For delta=1e-5, the current epsilon is: 0.61\n",
      "For delta=1e-5, the current epsilon is: 0.63\n",
      "For delta=1e-5, the current epsilon is: 0.65\n",
      "For delta=1e-5, the current epsilon is: 0.67\n",
      "Test accuracy after 1.0 epochs is: 0.835\n",
      "For delta=1e-5, the current epsilon is: 0.69\n",
      "For delta=1e-5, the current epsilon is: 0.71\n",
      "For delta=1e-5, the current epsilon is: 0.73\n",
      "For delta=1e-5, the current epsilon is: 0.75\n",
      "For delta=1e-5, the current epsilon is: 0.77\n",
      "Test accuracy after 1.5 epochs is: 0.836\n",
      "For delta=1e-5, the current epsilon is: 0.79\n",
      "For delta=1e-5, the current epsilon is: 0.81\n",
      "For delta=1e-5, the current epsilon is: 0.83\n",
      "For delta=1e-5, the current epsilon is: 0.85\n",
      "For delta=1e-5, the current epsilon is: 0.87\n",
      "Test accuracy after 2.0 epochs is: 0.835\n"
     ]
    }
   ],
   "source": [
    "!python 2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: DP logistic regression on realistic data\n",
    "\n",
    "Using the above code as a basis, build a DP logistic regression classifier for the UCI Adult data set (https://archive.ics.uci.edu/ml/datasets/Adult). (The data set is a standard benchmark data set that is available in various packages - feel free to use one of those.)\n",
    "\n",
    "How accurate classifier can you build to predict if an individual has an income of at most 50k, using DP with $\\epsilon=1, \\delta = 10^{-5}$? Report your accuracy on the separate test set not used in learning.\n",
    "\n",
    "Hint: the data set includes many categorical variables. In order to use these, you will need to use a one-hot encoding with $n-1$ variables used to denote $n$ values so that $k$th value is represented by value 1 in $k-1$st variable and zeros otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Your own problem in privacy-preserving machine learning\n",
    "\n",
    "State and solve your own problem related to privacy-preserving machine learning.\n",
    "\n",
    "You can use code available online, as long as you cite the source.\n",
    "\n",
    "You can for example try reproducing the results of some interesting paper using their data or your own data, try out some of the privacy attacks, or simply try the above examples using more complex models and/or on different data sets.\n",
    "\n",
    "If your problem is based on some previous problem, it should extend it in a non-trivial manner (not just running exact same code with new parameters or data).\n",
    "\n",
    "The evaluation of the project will take the difficulty of your chosen problem into account.\n",
    "\n",
    "This task is worth as much as two regular problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
